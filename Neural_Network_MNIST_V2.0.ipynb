{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network MNIST V2.0\n",
        "\n",
        "## Efficient MNIST Training Script\n",
        "**Target:** <25k parameters, >95% accuracy in 1 epoch\n",
        "\n",
        "This notebook demonstrates how to achieve high accuracy on MNIST with minimal parameters in just one epoch through careful architecture design and training optimization.\n",
        "\n",
        "### Features:\n",
        "- Highly efficient CNN architecture optimized for MNIST\n",
        "- Global Average Pooling instead of fully connected layers\n",
        "- Batch Normalization for better training stability\n",
        "- Gradient clipping for stable training\n",
        "- Cosine annealing learning rate scheduler\n",
        "- Parameter count: 24,048 (within 25k limit)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EfficientMNIST Model Class\n",
        "class EfficientMNIST(nn.Module):\n",
        "    \"\"\"\n",
        "    Highly efficient CNN for MNIST classification.\n",
        "    Achieves >95% accuracy in 1 epoch with <25k parameters.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=10, dropout_rate=0.1):\n",
        "        super(EfficientMNIST, self).__init__()\n",
        "        \n",
        "        # Efficient CNN architecture optimized for MNIST\n",
        "        # Total parameters: 24,048 (within 25k limit)\n",
        "        \n",
        "        # First conv block: 1->16 channels, 3x3 kernel\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        # Second conv block: 16->32 channels, 3x3 kernel\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        # Third conv block: 32->64 channels, 3x3 kernel\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        # Global average pooling instead of fully connected layers\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        \n",
        "        # Final classification layer (no bias to save parameters)\n",
        "        self.classifier = nn.Linear(64, num_classes, bias=False)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        # Max pooling\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # First conv block: 28x28 -> 14x14\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Second conv block: 14x14 -> 7x7\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        \n",
        "        # Third conv block: 7x7 -> 3x3 (after pooling)\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Global average pooling: 3x3 -> 1x1\n",
        "        x = self.global_avg_pool(x)\n",
        "        \n",
        "        # Flatten and classify\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def get_parameter_count(self):\n",
        "        \"\"\"Get parameter breakdown.\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        conv_params = sum(p.numel() for name, p in self.named_parameters() if 'conv' in name)\n",
        "        bn_params = sum(p.numel() for name, p in self.named_parameters() if 'bn' in name)\n",
        "        classifier_params = sum(p.numel() for name, p in self.named_parameters() if 'classifier' in name)\n",
        "        \n",
        "        return {\n",
        "            \"total\": total_params,\n",
        "            \"conv\": conv_params,\n",
        "            \"batch_norm\": bn_params,\n",
        "            \"classifier\": classifier_params\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading Function\n",
        "def load_mnist_data(batch_size=64, validation_split=0.1):\n",
        "    \"\"\"Load MNIST dataset.\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    \n",
        "    train_dataset = datasets.MNIST(\n",
        "        root='./data', train=True, download=True, transform=transform\n",
        "    )\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root='./data', train=False, download=True, transform=transform\n",
        "    )\n",
        "    \n",
        "    train_size = int((1 - validation_split) * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    \n",
        "    train_dataset, val_dataset = random_split(\n",
        "        train_dataset, [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Function\n",
        "def train_model(model, train_loader, val_loader, test_loader, epochs=1, lr=0.015):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.001)\n",
        "    \n",
        "    print(f\"Training on {device}\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += target.size(0)\n",
        "            \n",
        "            if batch_idx % 100 == 0:\n",
        "                current_acc = 100.0 * correct / total\n",
        "                print(f\"Batch {batch_idx}/{len(train_loader)}, \"\n",
        "                      f\"Loss: {loss.item():.4f}, Acc: {current_acc:.2f}%\")\n",
        "        \n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "                val_total += target.size(0)\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        train_acc = 100.0 * correct / total\n",
        "        val_acc = 100.0 * val_correct / val_total\n",
        "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "    \n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            test_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            test_total += target.size(0)\n",
        "    \n",
        "    test_acc = 100.0 * test_correct / test_total\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "    \n",
        "    if test_acc >= 95.0:\n",
        "        print(\"🎉 TARGET ACHIEVED: >95% accuracy in 1 epoch!\")\n",
        "    else:\n",
        "        print(f\"Target not met. Current accuracy: {test_acc:.2f}%\")\n",
        "    \n",
        "    return test_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main Training Execution\n",
        "print(\"=\" * 60)\n",
        "print(\"EFFICIENT MNIST TRAINING\")\n",
        "print(\"Target: <25k parameters, >95% accuracy in 1 epoch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create model\n",
        "model = EfficientMNIST()\n",
        "\n",
        "# Display model information\n",
        "params = model.get_parameter_count()\n",
        "print(f\"Model Architecture:\")\n",
        "print(f\"  Total parameters: {params['total']:,}\")\n",
        "print(f\"  Conv layers: {params['conv']:,}\")\n",
        "print(f\"  Batch norm: {params['batch_norm']:,}\")\n",
        "print(f\"  Classifier: {params['classifier']:,}\")\n",
        "print()\n",
        "\n",
        "if params['total'] > 25000:\n",
        "    print(\"⚠️  Model exceeds 25k parameter limit!\")\n",
        "else:\n",
        "    print(\"✅ Model within 25k parameter limit\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST Dataset\n",
        "print(\"Loading MNIST dataset...\")\n",
        "train_loader, val_loader, test_loader = load_mnist_data(batch_size=64)\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the Model\n",
        "start_time = time.time()\n",
        "test_accuracy = train_model(model, train_loader, val_loader, test_loader)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
