{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network MNIST Classifier V1.0\n",
        "\n",
        "This notebook implements a neural network classifier for MNIST digit recognition using PyTorch.\n",
        "\n",
        "## Features:\n",
        "- Custom ImageClassifier with convolutional and fully connected layers\n",
        "- Custom Dataset class for MNIST data loading\n",
        "- Data augmentation capabilities\n",
        "- Training and evaluation pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import struct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ImageClassifier Class - Fixed for MNIST (28x28 grayscale images)\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageClassifier, self).__init__()\n",
        "        \n",
        "        # Convolutional layers for feature extraction\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # First conv layer: 1 input channel (grayscale), 32 output channels, 3x3 kernel\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 28x28 -> 14x14\n",
        "            \n",
        "            # Second conv layer: 32 input channels, 64 output channels, 3x3 kernel\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 14x14 -> 7x7\n",
        "            \n",
        "            # Third conv layer: 64 input channels, 128 output channels, 3x3 kernel\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 7x7 -> 3x3 (with padding)\n",
        "        )\n",
        "        \n",
        "        # Fully connected layers for classification\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            # Input size: 128 * 3 * 3 = 1152 (for MNIST after conv layers)\n",
        "            nn.Linear(128 * 3 * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 10)  # 10 classes for MNIST digits (0-9)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Pass through convolutional layers\n",
        "        x = self.conv_layers(x)\n",
        "        \n",
        "        # Flatten the output for fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Pass through fully connected layers\n",
        "        x = self.fc_layers(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CustomDataset Class - Fixed implementation for MNIST\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images_path, labels_path, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with MNIST data files\n",
        "        \n",
        "        Args:\n",
        "            images_path (str): Path to MNIST images file\n",
        "            labels_path (str): Path to MNIST labels file\n",
        "            transform (callable, optional): Optional transform to be applied on a sample\n",
        "        \"\"\"\n",
        "        self.images_path = images_path\n",
        "        self.labels_path = labels_path\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Load MNIST data\n",
        "        self.images = self._load_images()\n",
        "        self.labels = self._load_labels()\n",
        "        \n",
        "        print(f\"Loaded {len(self.images)} images and {len(self.labels)} labels\")\n",
        "    \n",
        "    def _load_images(self):\n",
        "        \"\"\"Load MNIST images from binary file\"\"\"\n",
        "        with open(self.images_path, 'rb') as f:\n",
        "            # Read magic number\n",
        "            magic = struct.unpack('>I', f.read(4))[0]\n",
        "            # Read number of images\n",
        "            num_images = struct.unpack('>I', f.read(4))[0]\n",
        "            # Read image dimensions\n",
        "            rows = struct.unpack('>I', f.read(4))[0]\n",
        "            cols = struct.unpack('>I', f.read(4))[0]\n",
        "            \n",
        "            # Read image data\n",
        "            images = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "            images = images.reshape(num_images, rows, cols)\n",
        "            \n",
        "        return images\n",
        "    \n",
        "    def _load_labels(self):\n",
        "        \"\"\"Load MNIST labels from binary file\"\"\"\n",
        "        with open(self.labels_path, 'rb') as f:\n",
        "            # Read magic number\n",
        "            magic = struct.unpack('>I', f.read(4))[0]\n",
        "            # Read number of labels\n",
        "            num_labels = struct.unpack('>I', f.read(4))[0]\n",
        "            \n",
        "            # Read label data\n",
        "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "            \n",
        "        return labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples\"\"\"\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Generates one sample of data\"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        # Get image and label\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Convert to PIL Image for transforms\n",
        "        image = torchvision.transforms.ToPILImage()(image)\n",
        "        \n",
        "        # Apply transforms if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Default transform: convert to tensor and normalize\n",
        "            image = transforms.ToTensor()(image)\n",
        "        \n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Augmentation Transforms\n",
        "# Define transforms for training (with augmentation) and validation (without augmentation)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),  # Random rotation up to 10 degrees\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST normalization values\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST normalization values\n",
        "])\n",
        "\n",
        "print(\"Data augmentation transforms defined:\")\n",
        "print(\"Training transforms:\", train_transform)\n",
        "print(\"Validation transforms:\", val_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST Dataset\n",
        "# Define paths to MNIST data files\n",
        "data_dir = Path(\"data/MNIST/raw\")\n",
        "\n",
        "# Training data paths\n",
        "train_images_path = data_dir / \"train-images-idx3-ubyte\"\n",
        "train_labels_path = data_dir / \"train-labels-idx1-ubyte\"\n",
        "\n",
        "# Test data paths\n",
        "test_images_path = data_dir / \"t10k-images-idx3-ubyte\"\n",
        "test_labels_path = data_dir / \"t10k-labels-idx1-ubyte\"\n",
        "\n",
        "# Create datasets\n",
        "print(\"Loading training dataset...\")\n",
        "train_dataset = CustomDataset(\n",
        "    images_path=train_images_path,\n",
        "    labels_path=train_labels_path,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "print(\"Loading test dataset...\")\n",
        "test_dataset = CustomDataset(\n",
        "    images_path=test_images_path,\n",
        "    labels_path=test_labels_path,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Model, Loss Function, and Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create model\n",
        "model = ImageClassifier().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Print model summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model created successfully!\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model architecture:\")\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Train the model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "        \n",
        "        # Print progress\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(f'Epoch: {epoch}, Batch: {batch_idx}/{len(train_loader)}, '\n",
        "                  f'Loss: {loss.item():.4f}, Accuracy: {100.*correct/total:.2f}%')\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    \"\"\"Evaluate the model on test data\"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            \n",
        "            # Statistics\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "    \n",
        "    test_loss = test_loss / len(test_loader)\n",
        "    test_acc = 100. * correct / total\n",
        "    \n",
        "    return test_loss, test_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "num_epochs = 5\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Train the model\n",
        "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device, epoch+1)\n",
        "    \n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
        "    \n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "    \n",
        "    # Print epoch summary\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"  Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Training Results\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Training Loss')\n",
        "plt.plot(range(1, num_epochs+1), test_losses, 'r-', label='Test Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), train_accuracies, 'b-', label='Training Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), test_accuracies, 'r-', label='Test Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"Best Training Accuracy: {max(train_accuracies):.2f}%\")\n",
        "print(f\"Best Test Accuracy: {max(test_accuracies):.2f}%\")\n",
        "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Model on Sample Images\n",
        "def visualize_predictions(model, test_loader, device, num_samples=8):\n",
        "    \"\"\"Visualize model predictions on sample test images\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Get a batch of test data\n",
        "    data_iter = iter(test_loader)\n",
        "    images, labels = next(data_iter)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "    \n",
        "    # Convert to CPU for visualization\n",
        "    images = images.cpu()\n",
        "    labels = labels.cpu()\n",
        "    predicted = predicted.cpu()\n",
        "    \n",
        "    # Create subplot\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
        "        axes[i].set_title(f'True: {labels[i].item()}, Predicted: {predicted[i].item()}')\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize some predictions\n",
        "print(\"Sample predictions:\")\n",
        "visualize_predictions(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model_save_path = \"mnist_classifier_v1.0.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    'train_accuracies': train_accuracies,\n",
        "    'test_losses': test_losses,\n",
        "    'test_accuracies': test_accuracies,\n",
        "    'num_epochs': num_epochs,\n",
        "    'model_architecture': 'ImageClassifier'\n",
        "}, model_save_path)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n",
        "print(\"Model includes:\")\n",
        "print(\"- Model weights\")\n",
        "print(\"- Optimizer state\")\n",
        "print(\"- Training history\")\n",
        "print(\"- Model architecture info\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
